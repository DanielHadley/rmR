hotHoods <- d %>%
group_by(period, Community.Area) %>%
summarise(calls = n()) %>%
filter(Community.Area != "NA") %>%
spread(period, calls)
hotHoods <- d %>%
group_by(period, Community.Area) %>%
summarise(calls = n())
View(hotHoods)
hotHoods <- d %>%
group_by(period, Community.Area) %>%
summarise(calls = n()) %>%
filter(Community.Area != "NA")
View(hotHoods)
hotHoods <- d %>%
group_by(period, Community.Area) %>%
summarise(calls = n()) %>%
filter(Community.Area != "NA") %>%
spread(period, calls)
hotHoods <- d %>%
group_by(period, Community.Area) %>%
summarise(calls = n()) %>%
filter(Community.Area != "NA") %>%
spread(calls)
hotHoods <- d %>%
group_by(period, Community.Area) %>%
summarise(calls = n()) %>%
filter(Community.Area != "NA") %>%
spread(period
)
hotHoods <- d %>%
group_by(period, Community.Area) %>%
summarise(calls = n()) %>%
filter(Community.Area != "NA") %>%
spread(calls, period)
hotHoods <- d %>%
group_by(period, Community.Area) %>%
summarise(calls = n()) %>%
filter(Community.Area != "NA") %>%
spread(period, Community.Area)
hotHoods <- d %>%
group_by(period, Community.Area) %>%
summarise(calls = n()) %>%
filter(Community.Area != "NA")
View(hotHoods)
library(tidyr)
hotHoods <- d %>%
group_by(period, Community.Area) %>%
summarise(calls = n()) %>%
filter(Community.Area != "NA") %>%
spread(period, calls)
spread(hotHoods, period, calls)
hotHoods <- d %>%
group_by(period, Community.Area) %>%
summarise(calls = n()) %>%
filter(Community.Area != "NA") %>%
ungroup() %>%
spread(period, calls)
View(hotHoods)
hotHoods$Avg <- rowMeans(hotHoods[,c("PrevPer1", "PrevPer2", "PrevPer3")], na.rm=TRUE)
hotHoods$PerChng <- ((hotHoods$TrailingYear - hotHoods$Avg)) / hotHoods$Avg
hotHoods <- hotHoods %>%
mutate(quantile = ntile(Avg, 10)) %>%
filter(quantile > 1)
topNeighborhood <- subset(hotHoods, PerChng == max(PerChng))
bottomNeighborhood <- subset(hotHoods, PerChng == min(PerChng))
# Make a map for the blog post
topNeighborhoodData <- d %>%
filter(Community.Area == topNeighborhood$Community.Area)
bottomNeighborhoodData <- d %>%
filter(Community.Area == bottomNeighborhood$Community.Area)
# Geocode and then map
lon <- c(mean(topNeighborhoodData$Longitude))
lat <- c(mean(topNeighborhoodData$Latitude))
map.center <- data.frame(lon, lat)
SHmap <- qmap(c(lon=map.center$lon, lat=map.center$lat), source="google", zoom = 14, color='bw')
SHmap + geom_point(data=topNeighborhoodData, aes(y=Latitude, x=Longitude), size = 2, alpha = .3, bins = 26, color="red",)
View(hotHoods)
hotHoods <- hotHoods %>%
mutate(quantile = ntile(Avg, 10)) %>%
filter(quantile > 1) %>%
arrange(-Avg)
View(hotHoods)
hotHoods <- hotHoods %>%
mutate(quantile = ntile(Avg, 10)) %>%
filter(quantile > 1) %>%
arrange(-PerChng)
View(hotHoods)
hotHoods[1,i]
hotHoods[1,4]
hotHoods[5,i]
hotHoods[5,1]
i = 3
paste("../plots/Chicago_Rat_Map_Top_Neighborhood_2015_Neighborhood_", i,".png")
paste("../plots/Chicago_Rat_Map_Top_Neighborhood_2015_Neighborhood_", i,".png", sep="")
View(hotHoods)
library(dplyr)
library(truncnorm)
setwd("C:/Users/dhadley/Documents/GitHub/workNotebook/")
allData <- read.csv("./FirefightersVacation.csv")
# FY15 is not complete, so I will drop that
d <- allData %>%
select(-FY15Missed, -FY15Cost) %>%
mutate(date = as.Date(Day, format="%m/%d/%Y"))
d[is.na(d)] <- 0
d$costPerShift13 <- ifelse(d$FY13Cost == 0, 0, d$FY13Cost / d$FY13Missed)
d$costPerShift14 <- ifelse(d$FY14Cost == 0, 0, d$FY14Cost / d$FY14Missed)
hist(d$costPerShift13)
hist(d$costPerShift14)
# Most time we are not paying much overtime for missed shifts ^^^
#### This first section is to model the average OT cost per shift given how many shifts must be filled ####
# The problem, of course, is that there are other factors
# like sick shifts that come into play
# First combine the data to get averages and start the model
Thirteen <- d %>%
select(FY13Missed, Day, costPerShift13) %>%
rename(Missed = FY13Missed, cost = costPerShift13)
Fourteen <- d %>%
select(FY14Missed, Day, costPerShift14) %>%
rename(Missed = FY14Missed, cost = costPerShift14)
CostPerShift <- union(Thirteen, Fourteen)
rm(Thirteen, Fourteen)
# simple regression model to see what an average number of missed shifts will cost
byMissed <- CostPerShift %>%
group_by(Missed) %>%
summarise(avg = mean(cost),
min = min(cost),
max = max(cost),
stDev = sd(cost))
byMissed[1,] <- 0
reg1 <- lm(byMissed$avg~byMissed$Missed)
plot(byMissed$avg~byMissed$Missed)
abline(reg1)
byMissed$Predicted <- predict(reg1)
## new is the key for predicting cost per shift
x <- byMissed$Missed
y <- x + byMissed$avg
predict(lm(y ~ x))
new <- data.frame(x = seq(0,27))
reg2 <- predict(lm(y ~ x), new, se.fit = TRUE)
new$CostPerMissed <- reg2$fit
names(new)[1] <- "Missed"
# Now we will go back to d and try to model a year of shifting shifts
CostPerShift$OffPeak <- ifelse(CostPerShift$Missed > 5, "No", "Yes")
### A simulation of Off Peak
# First see what the actual dist looks like
OffPeak <- CostPerShift %>%
filter(OffPeak == "Yes")
summary(OffPeak$Missed)
sd(OffPeak$Missed)
hist(OffPeak$Missed)
### A simulation of Peak
# First see what the actual dist looks like
Peak <- CostPerShift %>%
filter(OffPeak == "No")
summary(Peak$Missed)
sd(Peak$Missed)
hist(Peak$Missed)
# Now simulate it
RegularYearSim <- function(nsims){
for (i in 1:nsims) {
OffPeakSim <- rtruncnorm(n=(nrow(OffPeak)/2), a=0, b=5, m=2.9, sd=1.6)
# I round so that I can combine with averages later
OffPeakSim <- round(OffPeakSim)
PeakSim <- rtruncnorm(n=(nrow(Peak)/2), a=6, b=26, m=10.6, sd=3.6)
# I round so that I can combine with averages later
PeakSim <- round(PeakSim)
OffPeakSim <- OffPeakSim  %>% as.data.frame()
PeakSim <- PeakSim  %>% as.data.frame()
OneYear <- rbind(OffPeakSim, PeakSim)
names(OneYear)[1] <- "Missed"
OneYearFinal <- merge(OneYear, new)
OneYearFinal$TotalCost <- OneYearFinal$Missed * OneYearFinal$CostPerMissed
totalCost = sum(OneYearFinal$TotalCost)
totalMissed = sum(OneYearFinal$Missed)
costPerShift = totalCost / totalMissed
print(paste("Total Cost: $", round(totalCost)))
print(paste("Total Missed Shifts:", totalMissed))
print(paste("Cost Per Shift:", round(costPerShift)))
}
}
ShiftedYearSim <- function(nsims, avgOffPeak, avgPeak){
for (i in 1:nsims) {
OffPeakSim <- rtruncnorm(n=(nrow(OffPeak)/2), a=0, b=5, m=avgOffPeak, sd=1.6)
# I round so that I can combine with averages later
OffPeakSim <- round(OffPeakSim)
PeakSim <- rtruncnorm(n=(nrow(Peak)/2), a=6, b=26, m=avgPeak, sd=3.6)
# I round so that I can combine with averages later
PeakSim <- round(PeakSim)
OffPeakSim <- OffPeakSim  %>% as.data.frame()
PeakSim <- PeakSim  %>% as.data.frame()
OneYear <- rbind(OffPeakSim, PeakSim)
names(OneYear)[1] <- "Missed"
OneYearFinal <- merge(OneYear, new)
OneYearFinal$TotalCost <- OneYearFinal$Missed * OneYearFinal$CostPerMissed
totalCost = sum(OneYearFinal$TotalCost)
totalMissed = sum(OneYearFinal$Missed)
costPerShift = totalCost / totalMissed
print(paste("Total Cost: $", round(totalCost)))
print(paste("Total Missed Shifts:", totalMissed))
print(paste("Cost Per Shift:", round(costPerShift)))
}
}
RegularYearSim(1)
ShiftedYearSim(1, 4, 9)
library(dplyr)
library(truncnorm)
setwd("C:/Users/dhadley/Documents/GitHub/workNotebook/")
allData <- read.csv("./FirefightersVacation.csv")
# FY15 is not complete, so I will drop that
d <- allData %>%
select(-FY15Missed, -FY15Cost) %>%
mutate(date = as.Date(Day, format="%m/%d/%Y"))
d[is.na(d)] <- 0
d$costPerShift13 <- ifelse(d$FY13Cost == 0, 0, d$FY13Cost / d$FY13Missed)
d$costPerShift14 <- ifelse(d$FY14Cost == 0, 0, d$FY14Cost / d$FY14Missed)
hist(d$costPerShift13)
hist(d$costPerShift14)
# Most time we are not paying much overtime for missed shifts ^^^
#### This first section is to model the average OT cost per shift given how many shifts must be filled ####
# The problem, of course, is that there are other factors
# like sick shifts that come into play
# First combine the data to get averages and start the model
Thirteen <- d %>%
select(FY13Missed, Day, costPerShift13) %>%
rename(Missed = FY13Missed, cost = costPerShift13)
Fourteen <- d %>%
select(FY14Missed, Day, costPerShift14) %>%
rename(Missed = FY14Missed, cost = costPerShift14)
CostPerShift <- union(Thirteen, Fourteen)
rm(Thirteen, Fourteen)
# simple regression model to see what an average number of missed shifts will cost
byMissed <- CostPerShift %>%
group_by(Missed) %>%
summarise(avg = mean(cost),
min = min(cost),
max = max(cost),
stDev = sd(cost))
byMissed[1,] <- 0
reg1 <- lm(byMissed$avg~byMissed$Missed)
plot(byMissed$avg~byMissed$Missed)
abline(reg1)
byMissed$Predicted <- predict(reg1)
library(dplyr)
library(truncnorm)
setwd("C:/Users/dhadley/Documents/GitHub/workNotebook/")
allData <- read.csv("./FirefightersVacation.csv")
# FY15 is not complete, so I will drop that
d <- allData %>%
select(-FY15Missed, -FY15Cost) %>%
mutate(date = as.Date(Day, format="%m/%d/%Y"))
d[is.na(d)] <- 0
d$costPerShift13 <- ifelse(d$FY13Cost == 0, 0, d$FY13Cost / d$FY13Missed)
d$costPerShift14 <- ifelse(d$FY14Cost == 0, 0, d$FY14Cost / d$FY14Missed)
hist(d$costPerShift13)
hist(d$costPerShift14)
# First combine the data to get averages and start the model
Thirteen <- d %>%
select(FY13Missed, Day, costPerShift13) %>%
rename(Missed = FY13Missed, cost = costPerShift13)
Fourteen <- d %>%
select(FY14Missed, Day, costPerShift14) %>%
rename(Missed = FY14Missed, cost = costPerShift14)
CostPerShift <- union(Thirteen, Fourteen)
rm(Thirteen, Fourteen)
byMissed <- CostPerShift %>%
group_by(Missed) %>%
summarise(avg = mean(cost),
min = min(cost),
max = max(cost),
stDev = sd(cost))
Thirteen <- d %>%
select(FY13Missed, Day, costPerShift13) %>%
rename(Missed = FY13Missed, cost = costPerShift13)
Fourteen <- d %>%
select(FY14Missed, Day, costPerShift14) %>%
rename(Missed = FY14Missed, cost = costPerShift14)
CostPerShift <- union(Thirteen, Fourteen)
View(Fourteen)
View(Fourteen)
CostPerShift <- union(Thirteen, Fourteen)
View(Fourteen)
View(Fourteen)
CostPerShift <- rbind(Thirteen, Fourteen)
rm(Thirteen, Fourteen)
# simple regression model to see what an average number of missed shifts will cost
byMissed <- CostPerShift %>%
group_by(Missed) %>%
summarise(avg = mean(cost),
min = min(cost),
max = max(cost),
stDev = sd(cost))
byMissed[1,] <- 0
reg1 <- lm(byMissed$avg~byMissed$Missed)
plot(byMissed$avg~byMissed$Missed)
abline(reg1)
byMissed$Predicted <- predict(reg1)
## new is the key for predicting cost per shift
x <- byMissed$Missed
y <- x + byMissed$avg
predict(lm(y ~ x))
new <- data.frame(x = seq(0,27))
reg2 <- predict(lm(y ~ x), new, se.fit = TRUE)
new$CostPerMissed <- reg2$fit
names(new)[1] <- "Missed"
# Now we will go back to d and try to model a year of shifting shifts
CostPerShift$OffPeak <- ifelse(CostPerShift$Missed > 5, "No", "Yes")
### A simulation of Off Peak
# First see what the actual dist looks like
OffPeak <- CostPerShift %>%
filter(OffPeak == "Yes")
summary(OffPeak$Missed)
sd(OffPeak$Missed)
hist(OffPeak$Missed)
### A simulation of Peak
# First see what the actual dist looks like
Peak <- CostPerShift %>%
filter(OffPeak == "No")
summary(Peak$Missed)
sd(Peak$Missed)
hist(Peak$Missed)
# Now simulate it
RegularYearSim <- function(nsims){
for (i in 1:nsims) {
OffPeakSim <- rtruncnorm(n=(nrow(OffPeak)/2), a=0, b=5, m=2.9, sd=1.6)
# I round so that I can combine with averages later
OffPeakSim <- round(OffPeakSim)
PeakSim <- rtruncnorm(n=(nrow(Peak)/2), a=6, b=26, m=10.6, sd=3.6)
# I round so that I can combine with averages later
PeakSim <- round(PeakSim)
OffPeakSim <- OffPeakSim  %>% as.data.frame()
PeakSim <- PeakSim  %>% as.data.frame()
OneYear <- rbind(OffPeakSim, PeakSim)
names(OneYear)[1] <- "Missed"
OneYearFinal <- merge(OneYear, new)
OneYearFinal$TotalCost <- OneYearFinal$Missed * OneYearFinal$CostPerMissed
totalCost = sum(OneYearFinal$TotalCost)
totalMissed = sum(OneYearFinal$Missed)
costPerShift = totalCost / totalMissed
print(paste("Total Cost: $", round(totalCost)))
print(paste("Total Missed Shifts:", totalMissed))
print(paste("Cost Per Shift:", round(costPerShift)))
}
}
ShiftedYearSim <- function(nsims, avgOffPeak, avgPeak){
for (i in 1:nsims) {
OffPeakSim <- rtruncnorm(n=(nrow(OffPeak)/2), a=0, b=5, m=avgOffPeak, sd=1.6)
# I round so that I can combine with averages later
OffPeakSim <- round(OffPeakSim)
PeakSim <- rtruncnorm(n=(nrow(Peak)/2), a=6, b=26, m=avgPeak, sd=3.6)
# I round so that I can combine with averages later
PeakSim <- round(PeakSim)
OffPeakSim <- OffPeakSim  %>% as.data.frame()
PeakSim <- PeakSim  %>% as.data.frame()
OneYear <- rbind(OffPeakSim, PeakSim)
names(OneYear)[1] <- "Missed"
OneYearFinal <- merge(OneYear, new)
OneYearFinal$TotalCost <- OneYearFinal$Missed * OneYearFinal$CostPerMissed
totalCost = sum(OneYearFinal$TotalCost)
totalMissed = sum(OneYearFinal$Missed)
costPerShift = totalCost / totalMissed
print(paste("Total Cost: $", round(totalCost)))
print(paste("Total Missed Shifts:", totalMissed))
print(paste("Cost Per Shift:", round(costPerShift)))
}
}
RegularYearSim(1)
ShiftedYearSim(1, 4, 9)
data <- read.xls(url(http://10.6.1.38/QED//partner/common/quicksearch/showexcel.jsp?d1=01/01/2015&d2=03/01/2015&submit.x=28&submit.y=9&package=CAD%2FPARTNER&title=Response+Times))
data <- read.xls(url("http://10.6.1.38/QED//partner/common/quicksearch/showexcel.jsp?d1=01/01/2015&d2=03/01/2015&submit.x=28&submit.y=9&package=CAD%2FPARTNER&title=Response+Times"))
data <- read.csv(url("http://10.6.1.38/QED//partner/common/quicksearch/showexcel.jsp?d1=01/01/2015&d2=03/01/2015&submit.x=28&submit.y=9&package=CAD%2FPARTNER&title=Response+Times"))
setwd("C:/Users/dhadley/Documents/GitHub/rmR")
# setwd("/home/pi/Github/rmR")
library(dplyr)
library(tidyr)
library(lubridate)
library(broom) # augments d with model variables
library(ggplot2)
library(ggmap)
set.seed(311)
# I can probably do each city by the delimited neighborhoods instead of k-means
# Except for Baltimore
#### Chicago ####
chi <- read.csv("./data/Chicago_rats.csv")
# Get dates ready
d <- chi %>%
mutate(date = as.Date(Creation.Date, format="%m/%d/%Y")) %>%
mutate(week = week(date),
year = year(date),
YearDay = yday(date))
# Trailing 365 - a better comparison than YTD when it's early in the year
lastDate <- max(d$date)
YearAgo <- lastDate - 365
TwoYearsAgo <- YearAgo - 365
ThreeYearsAgo <- TwoYearsAgo - 365
FourYearsAgo <- ThreeYearsAgo - 365
d$period <-
ifelse((d$date >= YearAgo), "TrailingYear",
ifelse((d$date >= TwoYearsAgo) & (d$date < YearAgo), "PrevPer1",
ifelse((d$date >= ThreeYearsAgo) & (d$date < TwoYearsAgo), "PrevPer2",
ifelse((d$date >= FourYearsAgo) & (d$date < ThreeYearsAgo), "PrevPer3",
"LongAgo"))))
## Use maps to inspect the neighborhoods ##
# Dot map
map.center <- geocode("Chicago, Il")
SHmap <- qmap(c(lon=map.center$lon, lat=map.center$lat), source="google", zoom = 12, color='bw')
SHmap + geom_point(data=d, aes(x=Longitude, y=Latitude, color=as.character(d$Community.Area)))
## See which one grew the most
hotHoods <- d %>%
group_by(period, Community.Area) %>%
summarise(calls = n()) %>%
filter(Community.Area != "NA") %>%
ungroup() %>%
spread(period, calls)
hotHoods$Avg <- rowMeans(hotHoods[,c("PrevPer1", "PrevPer2", "PrevPer3")], na.rm=TRUE)
hotHoods$PerChng <- ((hotHoods$TrailingYear - hotHoods$Avg)) / hotHoods$Avg
# Before selecting the top & bottom hoods, I drop the bottom 10% of avg total calls
# The outliers often have the most variance, and are less interesting
hotHoods <- hotHoods %>%
mutate(quantile = ntile(Avg, 10)) %>%
filter(quantile > 1) %>%
arrange(-PerChng)
for (i in 1:5) {
topNeighborhoodData <- d %>%
filter(Community.Area = hotHoods[i,1])
# Geocode and then map
lon <- c(mean(topNeighborhoodData$Longitude))
lat <- c(mean(topNeighborhoodData$Latitude))
map.center <- data.frame(lon, lat)
SHmap <- qmap(c(lon=map.center$lon, lat=map.center$lat), source="google", zoom = 14, color='bw')
SHmap + geom_point(data=topNeighborhoodData, aes(y=Latitude, x=Longitude), size = 2, alpha = .3, bins = 26, color="red",)
ggsave(paste("../plots/Chicago_Rat_Map_Top_Neighborhood_2015_Number_", i,".png", sep=""), dpi=200, width=4, height=4)
}
i = 3
topNeighborhoodData <- d %>%
filter(Community.Area = hotHoods[i,1])
hotHoods[i,1]
CA = hotHoods[i,1]
CA
CA = as.character(hotHoods[i,1])
CA
for (i in 1:5) {
CA = as.character(hotHoods[i,1])
topNeighborhoodData <- d %>%
filter(Community.Area = as.character(hotHoods[i,1]))
# Geocode and then map
lon <- c(mean(topNeighborhoodData$Longitude))
lat <- c(mean(topNeighborhoodData$Latitude))
map.center <- data.frame(lon, lat)
SHmap <- qmap(c(lon=map.center$lon, lat=map.center$lat), source="google", zoom = 14, color='bw')
SHmap + geom_point(data=topNeighborhoodData, aes(y=Latitude, x=Longitude), size = 2, alpha = .3, bins = 26, color="red",)
ggsave(paste("../plots/Chicago_Rat_Map_Top_Neighborhood_2015_Number_", i,".png", sep=""), dpi=200, width=4, height=4)
}
as.numeric(hotHoods[1,1])
for (i in 1:5) {
CA = as.character(hotHoods[i,1])
topNeighborhoodData <- d %>%
filter(Community.Area = as.numeric(hotHoods[i,1]))
# Geocode and then map
lon <- c(mean(topNeighborhoodData$Longitude))
lat <- c(mean(topNeighborhoodData$Latitude))
map.center <- data.frame(lon, lat)
SHmap <- qmap(c(lon=map.center$lon, lat=map.center$lat), source="google", zoom = 14, color='bw')
SHmap + geom_point(data=topNeighborhoodData, aes(y=Latitude, x=Longitude), size = 2, alpha = .3, bins = 26, color="red",)
ggsave(paste("../plots/Chicago_Rat_Map_Top_Neighborhood_2015_Number_", i,".png", sep=""), dpi=200, width=4, height=4)
}
for (i in 1:5) {
CA = as.numeric(hotHoods[i,1])
topNeighborhoodData <- d %>%
filter(Community.Area = CA)
# Geocode and then map
lon <- c(mean(topNeighborhoodData$Longitude))
lat <- c(mean(topNeighborhoodData$Latitude))
map.center <- data.frame(lon, lat)
SHmap <- qmap(c(lon=map.center$lon, lat=map.center$lat), source="google", zoom = 14, color='bw')
SHmap + geom_point(data=topNeighborhoodData, aes(y=Latitude, x=Longitude), size = 2, alpha = .3, bins = 26, color="red",)
ggsave(paste("../plots/Chicago_Rat_Map_Top_Neighborhood_2015_Number_", i,".png", sep=""), dpi=200, width=4, height=4)
}
CA = as.numeric(hotHoods[i,1])
CA
for (i in 1:5) {
CA = as.numeric(hotHoods[i,1])
topNeighborhoodData <- d %>%
filter(Community.Area = 3)
# Geocode and then map
lon <- c(mean(topNeighborhoodData$Longitude))
lat <- c(mean(topNeighborhoodData$Latitude))
map.center <- data.frame(lon, lat)
SHmap <- qmap(c(lon=map.center$lon, lat=map.center$lat), source="google", zoom = 14, color='bw')
SHmap + geom_point(data=topNeighborhoodData, aes(y=Latitude, x=Longitude), size = 2, alpha = .3, bins = 26, color="red",)
ggsave(paste("../plots/Chicago_Rat_Map_Top_Neighborhood_2015_Number_", i,".png", sep=""), dpi=200, width=4, height=4)
}
View(d)
for (i in 1:5) {
CA = as.numeric(hotHoods[i,1])
topNeighborhoodData <- d %>%
filter(Community.Area == CA)
# Geocode and then map
lon <- c(mean(topNeighborhoodData$Longitude))
lat <- c(mean(topNeighborhoodData$Latitude))
map.center <- data.frame(lon, lat)
SHmap <- qmap(c(lon=map.center$lon, lat=map.center$lat), source="google", zoom = 14, color='bw')
SHmap + geom_point(data=topNeighborhoodData, aes(y=Latitude, x=Longitude), size = 2, alpha = .3, bins = 26, color="red",)
ggsave(paste("../plots/Chicago_Rat_Map_Top_Neighborhood_2015_Number_", i,".png", sep=""), dpi=200, width=4, height=4)
}
CA = as.numeric(hotHoods[i,1])
topNeighborhoodData <- d %>%
filter(Community.Area == CA)
lon <- c(mean(topNeighborhoodData$Longitude))
lat <- c(mean(topNeighborhoodData$Latitude))
map.center <- data.frame(lon, lat)
SHmap <- qmap(c(lon=map.center$lon, lat=map.center$lat), source="google", zoom = 14, color='bw')
